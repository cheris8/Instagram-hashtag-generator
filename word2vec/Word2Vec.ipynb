{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 해시태그 전처리 모듈\n",
    "\n",
    "def PreprocessingHashtags(path):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    # 데이터 불러오기\n",
    "    data = pd.read_csv(path).iloc[:, 1:]\n",
    "    data = data[data['hashtags'].notnull()] # 해시태그 nan 제거\n",
    "    data = data.drop_duplicates('image_url') # 중복 행 제거\n",
    "    \n",
    "    # 한글, 영어 외 제거 후 우물 정(#) 기준으로 분리\n",
    "    p = re.compile(r'[가-힣a-zA-Z#]+')\n",
    "    data['hashtags_splitted'] = data['hashtags'].apply(lambda x: ''.join(p.findall(str(x))).split('#'))\n",
    "    \n",
    "    # 빈 해시태그 제거\n",
    "    data['hashtags_completed'] = ''\n",
    "    for i in range(len(data)):\n",
    "        ls = [word for word in data.iloc[i]['hashtags_splitted'] if word!='']\n",
    "        data['hashtags_completed'].iloc[i] = ls\n",
    "        \n",
    "    # 컬럼 삭제\n",
    "    data.drop(['hashtags', 'hashtags_splitted'], axis=1, inplace=True)\n",
    "        \n",
    "    # 컬럼명 변경\n",
    "    data.rename({'hashtags_completed':'hashtags'}, axis=1, inplace=True) \n",
    "    \n",
    "    # 인덱스 reset\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 출현빈도가 1회뿐인 해시태그 제거 모듈\n",
    "### 시간이 오래걸려서 전처리 모듈과 따로 분리했습니다.\n",
    "\n",
    "def PreprocessingHashtags_deletefreq(data):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # 데이터 가져오기\n",
    "    data = data\n",
    "    \n",
    "    # 출현빈도가 1회뿐인 해시태그 제거\n",
    "    hashtags_list = []\n",
    "    for x in range(len(data.hashtags)):\n",
    "        for y in range(len(data.hashtags[x])):\n",
    "            hashtags_list.append(data.hashtags[x][y])\n",
    "            \n",
    "    hashtags_set = set(hashtags_list)\n",
    "    hashtags_count = [hashtags_list.count(i) for i in hashtags_set]\n",
    "    hashtags_dict = dict(zip(hashtags_set, hashtags_count))\n",
    "\n",
    "    hashtags_df = pd.DataFrame()\n",
    "    hashtags_df['name'] = hashtags_dict.keys()\n",
    "    hashtags_df['count'] = hashtags_dict.values()\n",
    "    \n",
    "    hashtags_df = hashtags_df[hashtags_df['count'] >= 2]\n",
    "    hashtags_df = hashtags_df.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # 컬럼 변경 완료\n",
    "    new_hashtags = []\n",
    "    for x in range(len(data.hashtags)):\n",
    "        temp = []\n",
    "    \n",
    "        for y in range(len(data.hashtags[x])):    \n",
    "            if data.hashtags[x][y] in hashtags_list:\n",
    "                temp.append(data.hashtags[x][y])\n",
    "        new_hashtags.append(temp)\n",
    "        \n",
    "    data['hashtags'] = new_hashtags\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 토큰화 모듈\n",
    "\n",
    "def Tokenizer(data):\n",
    "    \n",
    "    from konlpy.tag import Okt\n",
    "    \n",
    "    # 데이터 가져오기\n",
    "    data = data\n",
    "    new_hashtags = data.hashtags.copy()\n",
    "    \n",
    "    # 토큰화\n",
    "    for i in range(len(new_hashtags)):\n",
    "        new_hashtags[i] = ' '.join(new_hashtags[i])\n",
    "    \n",
    "    okt = Okt()\n",
    "    tokenized = []\n",
    "\n",
    "    for sentence in new_hashtags:\n",
    "        tokens = okt.morphs(sentence)\n",
    "        tokenized.append(tokens)\n",
    "    \n",
    "    # 중복된 토큰 제거\n",
    "    new_tokenized = []\n",
    "    \n",
    "    for x in range(len(tokenized)):\n",
    "        temp = []\n",
    "    \n",
    "        for y in range(len(tokenized[x])):\n",
    "            if tokenized[x][y] not in temp:\n",
    "                temp.append(tokenized[x][y])\n",
    "    \n",
    "        new_tokenized.append(temp)\n",
    "    \n",
    "    return new_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word2Vec 학습 모듈\n",
    "\n",
    "def Word2Vec(tokenized, min_count=1, workers=8, size=70, window=40, sg=1, iter=10):\n",
    "    \n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    \n",
    "    # Word2Vec 학습\n",
    "\n",
    "    \"\"\"\n",
    "    sentences: 학습할 문장\n",
    "    min_count : 임베딩할 단어의 최소 빈도수\n",
    "    workers: 병렬 처리 스레드 수\n",
    "    size: word vector의 차원(임베딩 사이즈)\n",
    "    window: 윈도우 크기\n",
    "    sg: skip-gram 사용여부(1: 사용, other: CBOW)\n",
    "    iter: 학습횟수\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Word2Vec(sentences = tokenized,\n",
    "                 min_count = min_count,\n",
    "                 workers = workers,\n",
    "                 size = size,\n",
    "                 window = window,\n",
    "                 sg = sg,\n",
    "                 iter=iter)\n",
    "    \n",
    "    # 학습이 완료 되면 필요없는 메모리를 unload\n",
    "    model.init_sims(replace=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > 사용예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PreprocessingHashtags('data.csv')\n",
    "data = PreprocessingHashtags_deletefreq(data)\n",
    "tokenized = Tokenizer(data)\n",
    "model = Word2Vec(tokenized, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('감성', 0.9832465648651123),\n",
       " ('미치다', 0.9708598256111145),\n",
       " ('힐링', 0.9660876989364624),\n",
       " ('에', 0.9643505811691284),\n",
       " ('food', 0.9623520970344543)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = '여행'\n",
    "model.wv.most_similar(t,topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('소통', 0.9836547374725342),\n",
       " ('인스타', 0.9826146364212036),\n",
       " ('그램', 0.9819351434707642),\n",
       " ('스타', 0.9777668714523315),\n",
       " ('모델', 0.9748936891555786)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = '일상'\n",
    "model.wv.most_similar(t,topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name 지정 후 모델 저장 \n",
    "model.save('model/model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 사용하기\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model_1 = Word2Vec.load('model/model_1')\n",
    "model_full = Word2Vec.load('model/model_full') # 형태소 단위가 아니라 해시태그 하나를 토큰으로 두고 학습시킨 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1: \n",
      " [('바다', 0.8299013376235962), ('힐링', 0.7942224740982056), ('미치다', 0.7778294086456299), ('제주도', 0.7431781888008118), ('커피', 0.742461085319519), ('야경', 0.7047562599182129), ('제주', 0.7015856504440308), ('사진', 0.6972116231918335), ('드라이브', 0.6897653937339783), ('데이트', 0.6890974044799805)] \n",
      "\n",
      "model_full: \n",
      " [('여행스타그램', 0.8545773029327393), ('바다', 0.8091214299201965), ('travel', 0.7684853076934814), ('드라이브', 0.7474426031112671), ('제주도', 0.7408050298690796), ('떠나요', 0.7337548732757568), ('휴가', 0.7317897081375122), ('국내여행', 0.723529577255249), ('포항여행', 0.7225464582443237), ('세부', 0.7187522053718567)]\n"
     ]
    }
   ],
   "source": [
    "t = '여행'\n",
    "print('model_1: \\n {}'.format(model_1.wv.most_similar(t,topn=10)), '\\n')\n",
    "print('model_full: \\n {}'.format(model_full.wv.most_similar(t,topn=10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 해시태그 전처리 모듈\n",
    "\n",
    "def PreprocessingHashtags(path):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    # 데이터 불러오기\n",
    "    data = pd.read_csv(path).iloc[:, 1:]\n",
    "    data = data[data['hashtags'].notnull()] # 해시태그 nan 제거\n",
    "    data = data.drop_duplicates('image_url') # 중복 행 제거\n",
    "    \n",
    "    # 한글 외 제거 후 우물 정(#) 기준으로 분리\n",
    "    p = re.compile(r'[가-힣#]+')\n",
    "    data['hashtags_splitted'] = data['hashtags'].apply(lambda x: ''.join(p.findall(str(x))).split('#'))\n",
    "    \n",
    "    # 빈 해시태그 제거\n",
    "    data['hashtags_completed'] = ''\n",
    "    for i in range(len(data)):\n",
    "        ls = [word for word in data.iloc[i]['hashtags_splitted'] if word!='']\n",
    "        data['hashtags_completed'].iloc[i] = ls\n",
    "        \n",
    "    # 컬럼 삭제\n",
    "    data.drop(['hashtags', 'hashtags_splitted'], axis=1, inplace=True)\n",
    "        \n",
    "    # 컬럼명 변경\n",
    "    data.rename({'hashtags_completed':'hashtags'}, axis=1, inplace=True) \n",
    "    \n",
    "    # 인덱스 reset\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 출현빈도가 1회뿐인 해시태그 제거 모듈\n",
    "### 시간이 오래걸려서 전처리 모듈과 따로 분리했습니다.\n",
    "\n",
    "def PreprocessingHashtags_deletefreq(data):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # 데이터 가져오기\n",
    "    data = data\n",
    "    \n",
    "    # 출현빈도가 1회뿐인 해시태그 제거\n",
    "    hashtags_list = []\n",
    "    for x in range(len(data.hashtags)):\n",
    "        for y in range(len(data.hashtags[x])):\n",
    "            hashtags_list.append(data.hashtags[x][y])\n",
    "            \n",
    "    hashtags_set = set(hashtags_list)\n",
    "    hashtags_count = [hashtags_list.count(i) for i in hashtags_set]\n",
    "    hashtags_dict = dict(zip(hashtags_set, hashtags_count))\n",
    "\n",
    "    hashtags_df = pd.DataFrame()\n",
    "    hashtags_df['name'] = hashtags_dict.keys()\n",
    "    hashtags_df['count'] = hashtags_dict.values()\n",
    "    \n",
    "    hashtags_df = hashtags_df[hashtags_df['count'] >= 2]\n",
    "    hashtags_df = hashtags_df.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # 컬럼 변경 완료\n",
    "    new_hashtags = []\n",
    "    for x in range(len(data.hashtags)):\n",
    "        temp = []\n",
    "    \n",
    "        for y in range(len(data.hashtags[x])):    \n",
    "            if data.hashtags[x][y] in hashtags_list:\n",
    "                temp.append(data.hashtags[x][y])\n",
    "        new_hashtags.append(temp)\n",
    "        \n",
    "    data['hashtags'] = new_hashtags\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 토큰화 모듈\n",
    "\n",
    "def Tokenizer(data):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from ckonlpy.tag import Twitter\n",
    "    \n",
    "    twitter = Twitter()\n",
    "    \n",
    "    #사용자 사전 추가\n",
    "    txt = pd.read_csv('사용자 사전.txt', sep='\\n')\n",
    "    txt = txt['<사용자 사전>']\n",
    "    for line in txt:\n",
    "        twitter.add_dictionary(txt, 'Noun')\n",
    "    \n",
    "    # 데이터 가져오기\n",
    "    data = data\n",
    "    new_hashtags = data.hashtags.copy()\n",
    "    \n",
    "    # 토큰화\n",
    "    for i in range(len(new_hashtags)):\n",
    "        new_hashtags[i] = ' '.join(new_hashtags[i])\n",
    "    \n",
    "    tokenized = []\n",
    "\n",
    "    for sentence in new_hashtags:\n",
    "        tokens = twitter.morphs(sentence)\n",
    "        tokenized.append(tokens)\n",
    "    \n",
    "    # 연속된 중복 제거\n",
    "    new_tokenized = []\n",
    "    \n",
    "    for x in range(len(tokenized)):\n",
    "        temp = []\n",
    "\n",
    "        for y in range(len(tokenized[x])-1):\n",
    "            if tokenized[x][y] != tokenized[x][y+1]:\n",
    "                temp.append(tokenized[x][y])\n",
    "            \n",
    "        new_tokenized.append(temp)\n",
    "        \n",
    "    return new_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word2Vec 학습 모듈\n",
    "\n",
    "def Word2Vec(tokenized, min_count=1, workers=8, size=30, window=40, sg=1, iter=5):\n",
    "    \n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    \n",
    "    # Word2Vec 학습\n",
    "\n",
    "    \"\"\"\n",
    "    sentences: 학습할 문장\n",
    "    min_count : 임베딩할 단어의 최소 빈도수\n",
    "    workers: 병렬 처리 스레드 수\n",
    "    size: word vector의 차원(임베딩 사이즈)\n",
    "    window: 윈도우 크기\n",
    "    sg: skip-gram 사용여부(1: 사용, other: CBOW)\n",
    "    iter: 학습횟수\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Word2Vec(sentences = tokenized,\n",
    "                 min_count = min_count,\n",
    "                 workers = workers,\n",
    "                 size = size,\n",
    "                 window = window,\n",
    "                 sg = sg,\n",
    "                 iter=iter)\n",
    "    \n",
    "    # 학습이 완료 되면 필요없는 메모리를 unload\n",
    "    model.init_sims(replace=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > 사용예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 처음 토큰화 시 아래 다운로드\n",
    "# !git clone https://github.com/lovit/customized_konlpy.git\n",
    "# !pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUX\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "data = PreprocessingHashtags('data.csv')\n",
    "data = PreprocessingHashtags_deletefreq(data)\n",
    "tokenized = Tokenizer(data)\n",
    "model = Word2Vec(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('소통', 0.9483494162559509),\n",
       " ('그램', 0.8887966871261597),\n",
       " ('스타그램', 0.8861527442932129),\n",
       " ('인스타', 0.8684759140014648),\n",
       " ('맞팔', 0.8625321388244629),\n",
       " ('일리', 0.851641058921814),\n",
       " ('좋아요', 0.8473770022392273),\n",
       " ('업뎃', 0.8388894200325012),\n",
       " ('선팔', 0.8341192007064819),\n",
       " ('데일리', 0.8339321613311768)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = '일상'\n",
    "model.wv.most_similar(t,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('바보', 0.9055365324020386),\n",
       " ('도치', 0.880992591381073),\n",
       " ('아기', 0.8767290115356445),\n",
       " ('육아', 0.8716633319854736),\n",
       " ('베이비', 0.8673524260520935),\n",
       " ('개월', 0.858411967754364),\n",
       " ('성장기', 0.8518865704536438),\n",
       " ('닭띠', 0.8463918566703796),\n",
       " ('형제', 0.8420383930206299),\n",
       " ('쥐띠맘', 0.8355978727340698)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = '아들'\n",
    "model.wv.most_similar(t,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name 지정 후 모델 저장 \n",
    "model.save('model/model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 사용하기\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model_load = Word2Vec.load('model/model_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
